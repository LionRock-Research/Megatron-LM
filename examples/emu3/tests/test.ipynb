{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Megatron-LM/.pixi/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from megatron.training.tokenizer.tokenizer import _HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/Emu3-Gen/tokenization_emu3.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/data/models/Emu3-Gen/emu3_vision_tokens.txt' mode='r' encoding='UTF-8'>\n",
      "  vision_tokens = [t.strip() for t in open(special_tokens_file).readlines() if len(t.strip()) > 0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "tokenizer_megatron = _HuggingFaceTokenizer(\"/data/models/Emu3-Gen\")\n",
    "tokenizer_huggingface = AutoTokenizer.from_pretrained(\"/data/models/Emu3-Gen\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|extra_201|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_huggingface.eof_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['a portrait of young girl.', 'a portrait of young man.']\n",
    "assert tokenizer_megatron.tokenize(word_list) == [tokenizer_huggingface.encode(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtype = 'bfloat16'\n",
    "ATOL = {\n",
    "    'float32': 1e-3,\n",
    "    'bfloat16': 5e-2,\n",
    "    'float16': 5e-2,\n",
    "}\n",
    "RTOL = {\n",
    "    'float32': 1.3e-6,\n",
    "    'bfloat16': 1e-2,\n",
    "    'float16': 1e-2,\n",
    "}\n",
    "atol = ATOL[Dtype]\n",
    "rtol = RTOL[Dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/root/Megatron-LM/cache\"\n",
    "\n",
    "def inspect_output(hf_array, megatron_array):\n",
    "    print(f\"hf_array.shape: {hf_array.shape}, megatron_array.shape: {megatron_array.shape}\")\n",
    "    diff = np.abs(hf_array-megatron_array)\n",
    "    min_diff = diff.min()\n",
    "    max_diff = diff.max()\n",
    "    mean_diff = diff.mean()\n",
    "    print(f\"min_diff: {min_diff}, max_diff: {max_diff}, mean_diff: {mean_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "embedding dropout:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "embedding_hf = np.load(\"/root/Megatron-LM/cache/hf_model.embed_tokens.npy\")\n",
    "embedding_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.word_embeddings.npy\")\n",
    "embedding_flag = np.allclose(embedding_hf, embedding_megatron, atol=atol, rtol=rtol)\n",
    "print(\"embedding: \", embedding_flag)\n",
    "inspect_output(embedding_hf, embedding_megatron)\n",
    "# embedding dropout\n",
    "embedding_dropout_hf = np.load(\"/root/Megatron-LM/cache/hf_model.dropout.npy\")\n",
    "embedding_dropout_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.embedding_dropout.npy\")\n",
    "dropout_flag = np.allclose(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2), atol=atol, rtol=rtol)\n",
    "print(\"embedding dropout: \", dropout_flag)\n",
    "inspect_output(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear q:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00031974565354175866\n",
      "linear k:  True\n",
      "hf_array.shape: (1, 128, 1024), megatron_array.shape: (1, 128, 1024)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.0004661270650103688\n",
      "linear v:  True\n",
      "hf_array.shape: (1, 128, 1024), megatron_array.shape: (1, 128, 1024)\n",
      "min_diff: 0.0, max_diff: 0.001953125, mean_diff: 7.871985144447535e-05\n"
     ]
    }
   ],
   "source": [
    "# qkv_proj\n",
    "qkv_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_qkv_o0.npy\")\n",
    "q_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "k_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, 512:640].reshape(1, 128, -1)\n",
    "v_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, 640:].reshape(1, 128, -1)\n",
    "q_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.q_proj.npy\")\n",
    "k_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.k_proj.npy\")\n",
    "v_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.v_proj.npy\")\n",
    "layer_norm_flag = np.allclose(q_megatron, q_hf, atol=atol, rtol=rtol) \n",
    "print(\"linear q: \", layer_norm_flag)\n",
    "inspect_output(q_megatron, q_hf)\n",
    "layer_norm_flag = np.allclose(k_megatron, k_hf, atol=atol, rtol=rtol)\n",
    "print(\"linear k: \", layer_norm_flag)\n",
    "inspect_output(k_megatron, k_hf)\n",
    "layer_norm_flag = np.allclose(v_megatron, v_hf, atol=atol, rtol=rtol)\n",
    "print(\"linear v: \", layer_norm_flag)\n",
    "inspect_output(v_megatron, v_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0009765625, mean_diff: 5.376178251026431e-06\n",
      "layer 1:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00011902677942998707\n",
      "layer 2:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.0008853924227878451\n",
      "layer 3:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.228515625, mean_diff: 0.002636928576976061\n",
      "layer 4:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.1171875, mean_diff: 0.0017968036700040102\n",
      "layer 5:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.09375, mean_diff: 0.001739551080390811\n",
      "layer 6:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.109375, mean_diff: 0.001760266488417983\n",
      "layer 7:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.09375, mean_diff: 0.0019514535088092089\n",
      "layer 8:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.1015625, mean_diff: 0.0023446399718523026\n",
      "layer 9:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0634765625, mean_diff: 0.0021762812975794077\n",
      "layer 10:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.068359375, mean_diff: 0.002449435880407691\n",
      "layer 11:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.078125, mean_diff: 0.0028809502255171537\n",
      "layer 12:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0546875, mean_diff: 0.00276570743881166\n",
      "layer 13:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.078125, mean_diff: 0.0034735335502773523\n",
      "layer 14:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.08984375, mean_diff: 0.0034776143729686737\n",
      "layer 15:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.15625, mean_diff: 0.004402209538966417\n",
      "layer 16:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.1953125, mean_diff: 0.00675676204264164\n",
      "layer 17:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.15625, mean_diff: 0.005355854518711567\n",
      "layer 18:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.21875, mean_diff: 0.003834076691418886\n",
      "layer 19:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0546875, mean_diff: 0.0031811476219445467\n",
      "layer 20:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.162109375, mean_diff: 0.0026334007270634174\n",
      "layer 21:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.125, mean_diff: 0.0030092711094766855\n",
      "layer 22:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.171875, mean_diff: 0.0035027931444346905\n",
      "layer 23:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.21875, mean_diff: 0.003668825142085552\n",
      "layer 24:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.125, mean_diff: 0.0032730058301240206\n",
      "layer 25:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.109375, mean_diff: 0.0033451670315116644\n",
      "layer 26:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.103515625, mean_diff: 0.0034923749044537544\n",
      "layer 27:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.2578125, mean_diff: 0.003934639506042004\n",
      "layer 28:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.20654296875, mean_diff: 0.004196650348603725\n",
      "layer 29:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 5.4765625, mean_diff: 0.00785041507333517\n",
      "layer 30:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 7.65625, mean_diff: 0.0131340641528368\n",
      "layer 31:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.125, mean_diff: 0.013283631764352322\n"
     ]
    }
   ],
   "source": [
    "# attention output\n",
    "for i in range(32):\n",
    "    attention_output_hf = np.load(f\"{cache_dir}/hf_model.layers.{i}.self_attn_o0.npy\")\n",
    "    attention_output_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.{i}.self_attention_o0.npy\").transpose(1,0,2)\n",
    "    print(f\"layer {i}: \", np.allclose(attention_output_hf, attention_output_megatron, atol=atol, rtol=rtol))\n",
    "    inspect_output(attention_output_hf, attention_output_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 184622), megatron_array.shape: (1, 128, 184622)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_diff: 0.0, max_diff: 2.59375, mean_diff: 0.09347893297672272\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "output_hf = np.load(f\"{cache_dir}/hf_lm_head.npy\")\n",
    "output_megatron = np.load(f\"{cache_dir}/megatron_output_layer_o0.npy\").transpose(1,0,2)\n",
    "inspect_output(output_hf, output_megatron)\n",
    "np.allclose(output_hf, output_megatron, atol=atol, rtol=rtol)\n",
    "\n",
    "output_hf_label = output_hf.argmax(axis=-1)\n",
    "output_megatron_label = output_megatron.argmax(axis=-1)\n",
    "print(np.sum(output_hf_label!=output_megatron_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (128, 128), megatron_array.shape: (128, 128)\n",
      "min_diff: 0.0, max_diff: 0.0019521117210388184, mean_diff: 0.0004719264688901603\n",
      "hf_array.shape: (128, 128), megatron_array.shape: (128, 128)\n",
      "min_diff: 0.0, max_diff: 0.001952826976776123, mean_diff: 0.0003145454975310713\n"
     ]
    }
   ],
   "source": [
    "# rotary embedding\n",
    "cos_hf = np.load(f\"{cache_dir}/hf_model.layers.1.self_attn.rotary_emb_o0.npy\")\n",
    "sin_hf = np.load(f\"{cache_dir}/hf_model.layers.1.self_attn.rotary_emb_o1.npy\")\n",
    "cos_megatron = np.cos(np.load(f\"{cache_dir}/megatron_rotary_pos_emb.npy\")).reshape(128, 128)\n",
    "sin_megatron = np.sin(np.load(f\"{cache_dir}/megatron_rotary_pos_emb.npy\")).reshape(128, 128)\n",
    "inspect_output(cos_hf, cos_megatron)\n",
    "inspect_output(sin_hf, sin_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_megatron = np.load(f'{cache_dir}/megatron_rotary_pos_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.from_numpy(q_hf).cuda().reshape(1, 128, 32, -1).transpose(1, 2)\n",
    "k = torch.from_numpy(k_hf).cuda().reshape(1, 128, 8, -1).transpose(1, 2)\n",
    "v = torch.from_numpy(v_hf).cuda().reshape(1, 128, 8, -1).transpose(1, 2)\n",
    "cos, sin = torch.from_numpy(np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.rotary_emb_o0.npy\")), torch.from_numpy(np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.rotary_emb_o1.npy\"))\n",
    "q, k = apply_rotary_pos_emb(q, k, cos.cuda(), sin.cuda(), position_ids=torch.arange(128, device=\"cuda\").unsqueeze(0))\n",
    "k = k[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "v = v[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "\n",
    "output = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=True).permute(0, 2, 1, 3).reshape(1, 128, -1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 184622), megatron_array.shape: (1, 128, 184622)\n",
      "min_diff: 0.0, max_diff: 2.59375, mean_diff: 0.09347893297672272\n"
     ]
    }
   ],
   "source": [
    "inspect_output(output_hf, output_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n"
     ]
    }
   ],
   "source": [
    "rms_norm_output_megatron = np.load(f\"{cache_dir}/megatron_rms_norm.npy\")\n",
    "rms_norm_output_hf = np.load(f\"{cache_dir}/hf_rms_norm.npy\")\n",
    "inspect_output(rms_norm_output_hf, rms_norm_output_megatron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformer_engine.pytorch as te\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG \"] = \":16:8\"\n",
    "\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Emu3RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Emu3RMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class TorchRMSNorm(nn.Module):\n",
    "    def __init__(self, in_features, zero_centered_gamma, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.in_features = in_features\n",
    "        self.zero_centered_gamma = zero_centered_gamma\n",
    "\n",
    "        initial_value = torch.ones(in_features) if zero_centered_gamma else torch.zeros(in_features)\n",
    "        self.weight = nn.Parameter(initial_value)\n",
    "        self.register_parameter(\"weight\", self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x2 = torch.sum(x.float() ** 2, dim=-1, keepdim=True)\n",
    "        d_x = self.in_features\n",
    "\n",
    "        rms_x2 = norm_x2 / d_x + self.eps\n",
    "        r_rms_x = rms_x2 ** (-1.0 / 2)\n",
    "        x_normed = x * r_rms_x\n",
    "\n",
    "        w = self.weight.float()\n",
    "        if self.zero_centered_gamma:\n",
    "            w = 1 + w\n",
    "        return (w * x_normed).to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082487804349512\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00021006364841014147\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.0002690280962269753\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n"
     ]
    }
   ],
   "source": [
    "# case I, float32 input\n",
    "\n",
    "input = np.load(f\"{cache_dir}/hf_model.dropout.npy\")\n",
    "input = torch.from_numpy(input).cuda().bfloat16()\n",
    "\n",
    "weight = np.load(f\"{cache_dir}/megatron_rmsnorm_weight.npy\")\n",
    "weight = torch.from_numpy(weight).cuda().bfloat16()\n",
    "\n",
    "hf_rms_norm = Emu3RMSNorm(4096, eps=1e-5)\n",
    "hf_rms_norm.weight.requires_grad = False\n",
    "\n",
    "te_rms_norm = te.RMSNorm(4096, eps=1e-5, params_dtype=torch.bfloat16)\n",
    "te_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm = torch.nn.RMSNorm(4096, eps=1e-5)\n",
    "torch_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm_2 = TorchRMSNorm(4096, zero_centered_gamma=False, eps=1e-5)\n",
    "torch_rms_norm_2.weight.requires_grad = False\n",
    "\n",
    "hf_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "te_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm_2.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_hf = hf_rms_norm(input).cpu().float().numpy()\n",
    "    output_te = te_rms_norm(input).cpu().float().numpy()\n",
    "    output_torch = torch_rms_norm(input).cpu().float().numpy()\n",
    "    output_torch_2 = torch_rms_norm_2(input).cpu().float().numpy()\n",
    "inspect_output(output_hf, output_te)\n",
    "inspect_output(output_hf, output_torch)\n",
    "inspect_output(output_hf, output_torch_2)\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qkv Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input = hf_rms_norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00014284173084888607\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00014285619545262307\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0078125, mean_diff: 3.199484126525931e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "te_weight = np.load(f'{cache_dir}/megatron_qkv_linear_weight.npy')\n",
    "hf_q_weight = np.load(f'{cache_dir}/hf_q_linear_weight.npy')\n",
    "\n",
    "\n",
    "te_linear = te.Linear(6144, 4096, params_dtype=torch.bfloat16, bias=False, device=\"cuda\")\n",
    "te_linear.weight.requires_grad=False\n",
    "te_linear.weight = torch.nn.Parameter(torch.from_numpy(te_weight).cuda().bfloat16())\n",
    "\n",
    "hf_q_linear = torch.nn.Linear(4096, 4096, bias=False, dtype=torch.bfloat16)\n",
    "hf_q_linear.weight.requires_grad=False\n",
    "hf_q_linear.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda().bfloat16())\n",
    "\n",
    "hf_q_linear_fp32 = torch.nn.Linear(4096, 4096, bias=False)\n",
    "hf_q_linear_fp32.weight.requires_grad=False\n",
    "hf_q_linear_fp32.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda())\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_te = te_linear(input).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "    q_hf = hf_q_linear(input)\n",
    "    q_hf_fp32 = hf_q_linear_fp32(input.float()).bfloat16()\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf.cpu().float().numpy())\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf_fp32.cpu().float().numpy()) \n",
    "inspect_output(q_hf.cpu().float().numpy(), q_hf_fp32.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (4096, 4096), megatron_array.shape: (4096, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "te_q_linear_weight = np.load(f'{cache_dir}/megatron_qkv_linear_weight.npy').reshape(8, -1, 4096)[:, :512, :].reshape(4096, 4096)\n",
    "hf_q_linear_weight = np.load(f'{cache_dir}/hf_q_linear_weight.npy')\n",
    "inspect_output(te_q_linear_weight, hf_q_linear_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00020105975272599608\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n",
    "te_linear_q = te.Linear(4096, 4096, params_dtype=torch.bfloat16, bias=False, device=\"cuda\")\n",
    "te_linear_q.weight.requires_grad=False\n",
    "te_linear_q.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda().bfloat16())\n",
    "with torch.no_grad():\n",
    "    q_te = te_linear_q(input)\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0078125, mean_diff: 6.887059385007888e-07\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n",
    "torch_linear_qkv = torch.nn.Linear(6144, 4096, bias=False, dtype=torch.bfloat16)\n",
    "torch_linear_qkv.weight.requires_grad=False\n",
    "torch_linear_qkv.weight = torch.nn.Parameter(torch.from_numpy(te_weight).cuda().bfloat16())\n",
    "with torch.no_grad():\n",
    "    q_torch = torch_linear_qkv(input).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "inspect_output(q_torch.cpu().float().numpy(), q_hf.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.permute(2, 0, 1, 3)\n",
    "k = k.permute(2, 0, 1, 3)\n",
    "v = v.permute(2, 0, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_megatron = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o0.npy\")).cuda()\n",
    "k_megatron = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o1.npy\")).cuda()\n",
    "v_megatron = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o2.npy\")).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = te.DotProductAttention(num_attention_heads=32, kv_channels=128, attention_dropout=0.0, attn_mask_type=\"causal\")\n",
    "output_te = attn(q, k, v).transpose(1, 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = te.DotProductAttention(num_attention_heads=32, kv_channels=128, num_gqa_groups=8, attention_dropout=0.0, attn_mask_type=\"causal\")\n",
    "output_te = attn(q_megatron, k_megatron, v_megatron).transpose(1, 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_te[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_attn_output_megatron[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082487804349512\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.011625289916992188, mean_diff: 0.0002484717406332493\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0077877044677734375, mean_diff: 0.00018516556883696467\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Megatron-LM/.pixi/envs/default/lib/python3.10/site-packages/transformer_engine/pytorch/module/rmsnorm.py:193: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at ../torch/csrc/autograd/init.cpp:787.)\n",
      "  self.activation_dtype = torch.get_autocast_gpu_dtype()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# case I, float32 input\n",
    "\n",
    "input = np.load(f\"{cache_dir}/hf_model.dropout.npy\")\n",
    "input = torch.from_numpy(input).cuda().bfloat16()\n",
    "\n",
    "weight = np.load(f\"{cache_dir}/megatron_rmsnorm_weight.npy\")\n",
    "weight = torch.from_numpy(weight).cuda().bfloat16()\n",
    "\n",
    "hf_rms_norm = Emu3RMSNorm(4096, eps=1e-5)\n",
    "hf_rms_norm.weight.requires_grad = False\n",
    "\n",
    "te_rms_norm = te.RMSNorm(4096, eps=1e-5)\n",
    "te_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm = torch.nn.RMSNorm(4096, eps=1e-5)\n",
    "torch_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm_2 = TorchRMSNorm(4096, zero_centered_gamma=False, eps=1e-5)\n",
    "torch_rms_norm_2.weight.requires_grad = False\n",
    "\n",
    "hf_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "te_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm_2.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        output_hf = hf_rms_norm(input).cpu().float().numpy()\n",
    "        output_te = te_rms_norm(input).cpu().float().numpy()\n",
    "        output_torch = torch_rms_norm(input).cpu().float().numpy()\n",
    "        output_torch_2 = torch_rms_norm_2(input).cpu().float().numpy()\n",
    "inspect_output(output_hf, output_te)\n",
    "inspect_output(output_hf, output_torch)\n",
    "inspect_output(output_hf, output_torch_2)\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 10.078125, mean_diff: 0.0809398666024208\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 10.078125, mean_diff: 0.08094507455825806\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 10.078125, mean_diff: 0.08098115772008896\n"
     ]
    }
   ],
   "source": [
    "def fp32_rms_norm(x, weight):\n",
    "    x = x.to(torch.float32)\n",
    "    variance = x.pow(2).mean(-1, keepdim=True)\n",
    "    weight = weight.to(torch.float32)\n",
    "    x = x * torch.rsqrt(variance + 1e-5)\n",
    "    return (weight * x).bfloat16()\n",
    "output_fp32 = fp32_rms_norm(input, weight).cpu().float().numpy()\n",
    "inspect_output(output_fp32, output_te)\n",
    "inspect_output(output_fp32, output_hf)\n",
    "inspect_output(output_fp32, output_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.20117188,  0.06689453,  0.02563477, ...,  0.27539062,\n",
       "         -0.20507812,  0.15234375],\n",
       "        [ 0.15039062, -0.37109375,  0.34570312, ...,  0.44726562,\n",
       "         -0.11279297, -0.05859375],\n",
       "        [ 0.00604248, -0.01586914,  0.09619141, ..., -0.2890625 ,\n",
       "         -0.01452637,  0.140625  ],\n",
       "        ...,\n",
       "        [-0.33007812,  0.01037598,  0.12011719, ..., -0.20605469,\n",
       "         -0.02709961, -0.00515747],\n",
       "        [-0.296875  ,  0.10498047,  0.20800781, ..., -0.21484375,\n",
       "          0.06835938,  0.46289062],\n",
       "        [ 0.13183594, -0.09814453,  0.18847656, ..., -0.11767578,\n",
       "         -0.12207031, -0.515625  ]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.20214844,  0.06689453,  0.02563477, ...,  0.27539062,\n",
       "         -0.20507812,  0.15234375],\n",
       "        [ 0.15039062, -0.37109375,  0.34570312, ...,  0.44726562,\n",
       "         -0.11279297, -0.05859375],\n",
       "        [ 0.006073  , -0.01586914,  0.09619141, ..., -0.28710938,\n",
       "         -0.01452637,  0.14160156],\n",
       "        ...,\n",
       "        [-0.33007812,  0.01031494,  0.12011719, ..., -0.20605469,\n",
       "         -0.02722168, -0.00515747],\n",
       "        [-0.296875  ,  0.10498047,  0.20703125, ..., -0.21484375,\n",
       "          0.06884766,  0.46289062],\n",
       "        [ 0.13183594, -0.09765625,  0.18847656, ..., -0.11767578,\n",
       "         -0.12207031, -0.51953125]]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_te"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
