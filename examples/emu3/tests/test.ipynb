{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Megatron-LM/.pixi/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from megatron.training.tokenizer.tokenizer import _HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/Emu3-Gen/tokenization_emu3.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/data/models/Emu3-Gen/emu3_vision_tokens.txt' mode='r' encoding='UTF-8'>\n",
      "  vision_tokens = [t.strip() for t in open(special_tokens_file).readlines() if len(t.strip()) > 0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "tokenizer_megatron = _HuggingFaceTokenizer(\"/data/models/Emu3-Gen\")\n",
    "tokenizer_huggingface = AutoTokenizer.from_pretrained(\"/data/models/Emu3-Gen\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|extra_201|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_huggingface.eof_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['a portrait of young girl.', 'a portrait of young man.']\n",
    "assert tokenizer_megatron.tokenize(word_list) == [tokenizer_huggingface.encode(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/root/Megatron-LM/cache\"\n",
    "\n",
    "def inspect_output(hf_array, megatron_array):\n",
    "    print(f\"hf_array.shape: {hf_array.shape}, megatron_array.shape: {megatron_array.shape}\")\n",
    "    diff = np.abs(hf_array-megatron_array)\n",
    "    min_diff = diff.min()\n",
    "    max_diff = diff.max()\n",
    "    mean_diff = diff.mean()\n",
    "    print(f\"min_diff: {min_diff}, max_diff: {max_diff}, mean_diff: {mean_diff}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding:  True\n",
      "embedding dropout:  True\n"
     ]
    }
   ],
   "source": [
    "embedding_hf = np.load(\"/root/Megatron-LM/cache/hf_model.embed_tokens.npy\")\n",
    "embedding_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.word_embeddings.npy\")\n",
    "embedding_flag = np.allclose(embedding_hf, embedding_megatron, atol=5e-5, rtol=1e-5)\n",
    "print(\"embedding: \", embedding_flag)\n",
    "if not embedding_flag:\n",
    "    inspect_output(embedding_hf, embedding_megatron)\n",
    "# embedding dropout\n",
    "embedding_dropout_hf = np.load(\"/root/Megatron-LM/cache/hf_model.dropout.npy\")\n",
    "embedding_dropout_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.embedding_dropout.npy\")\n",
    "dropout_flag = np.allclose(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2), atol=5e-5, rtol=1e-5)\n",
    "print(\"embedding dropout: \", dropout_flag)\n",
    "if not dropout_flag:\n",
    "    inspect_output(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_norm:  True\n"
     ]
    }
   ],
   "source": [
    "# qkv_proj\n",
    "qkv_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_qkv.npy_o0.npy\")\n",
    "q_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "q_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.q_proj.npy\")\n",
    "layer_norm_flag = np.allclose(q_megatron, q_hf, atol=5e-5, rtol=1e-5)\n",
    "print(\"layer_norm: \", layer_norm_flag)\n",
    "if not layer_norm_flag:\n",
    "    inspect_output(q_megatron, q_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "hf_array.shape: (128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.00046753883361816406, mean_diff: 1.3898101315135136e-05\n"
     ]
    }
   ],
   "source": [
    "x = np.load('/root/Megatron-LM/cache/hf_model.layers.0.input_layernorm.npy')\n",
    "\n",
    "# megatron qkv\n",
    "megatron_qkv = x@megatron_linear_qkv.T\n",
    "megatron_q = megatron_qkv.reshape(128, 8, -1)[:, :, :512].reshape(128, -1)\n",
    "hf_q = x@hf_linear_q.T\n",
    "inspect_output(megatron_q, hf_q)\n",
    "inspect_output(megatron_q, q_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.9073486328125e-06, mean_diff: 3.73696664723866e-08\n"
     ]
    }
   ],
   "source": [
    "inspect_output(q_hf, hf_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_out:  [[-0.20171243  0.06703024  0.0255844  ...  0.27597302 -0.20493615\n",
    "   0.15210561]\n",
    " [ 0.15066259 -0.37075084  0.34638226 ...  0.44702706 -0.1128664\n",
    "  -0.05870222]\n",
    " [ 0.00606197 -0.01582064  0.09621283 ... -0.28794834 -0.0145349\n",
    "   0.14136854]\n",
    " ...\n",
    " [-0.32960933  0.01034221  0.12024365 ... -0.20597263 -0.02718414\n",
    "  -0.00516073]\n",
    " [-0.29591063  0.10484271  0.207427   ... -0.21518306  0.0686103\n",
    "   0.46210706]\n",
    " [ 0.13179992 -0.09774409  0.18862736 ... -0.11785763 -0.1223011\n",
    "  -0.5176106 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 184622), megatron_array.shape: (1, 128, 184622)\n",
      "min_diff: 0.0, max_diff: 22.800884246826172, mean_diff: 3.6361570358276367\n"
     ]
    }
   ],
   "source": [
    "output_hf = np.load(\"output_hf.npy\")\n",
    "output_megatron = np.load(\"output_megatron.npy\")\n",
    "inspect_output(output_hf, output_megatron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import transformer_engine.pytorch as te\n",
    "import numpy as np\n",
    "\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.load('/root/Megatron-LM/cache/hf_model.layers.0.input_layernorm.npy')).cuda()\n",
    "\n",
    "te_mlp = te.Linear(4096, 6144, bias=False)\n",
    "with torch.no_grad():\n",
    "    te_mlp.weight.copy_(torch.from_numpy(megatron_linear_qkv).cuda())\n",
    "\n",
    "torch_mlp = torch.nn.Linear(4096, 6144, bias=False, device='cuda')\n",
    "with torch.no_grad():\n",
    "    torch_mlp.weight.copy_(torch.from_numpy(megatron_linear_qkv).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (128, 4096), megatron_array.shape: (128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "q_te = te_mlp(x).detach().cpu().numpy().reshape(128, 8, -1)[:, :, :512].reshape(128, -1)\n",
    "q_torch = torch_mlp(x).detach().cpu().numpy().reshape(128, 8, -1)[:, :, :512].reshape(128, -1)\n",
    "inspect_output(q_te, q_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
