{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Megatron-LM/.pixi/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from megatron.training.tokenizer.tokenizer import _HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/Emu3-Gen/tokenization_emu3.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/data/models/Emu3-Gen/emu3_vision_tokens.txt' mode='r' encoding='ANSI_X3.4-1968'>\n",
      "  vision_tokens = [t.strip() for t in open(special_tokens_file).readlines() if len(t.strip()) > 0]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "tokenizer_megatron = _HuggingFaceTokenizer(\"/data/models/Emu3-Gen\")\n",
    "tokenizer_huggingface = AutoTokenizer.from_pretrained(\"/data/models/Emu3-Gen\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|extra_201|>'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_huggingface.eof_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['a portrait of young girl.', 'a portrait of young man.']\n",
    "assert tokenizer_megatron.tokenize(word_list) == [tokenizer_huggingface.encode(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtype = 'bfloat16'\n",
    "ATOL = {\n",
    "    'float32': 1e-3,\n",
    "    'bfloat16': 5e-2,\n",
    "    'float16': 5e-2,\n",
    "}\n",
    "RTOL = {\n",
    "    'float32': 1.3e-6,\n",
    "    'bfloat16': 1e-2,\n",
    "    'float16': 1e-2,\n",
    "}\n",
    "atol = ATOL[Dtype]\n",
    "rtol = RTOL[Dtype]\n",
    "QKV_LINEAR_FUSION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/root/Megatron-LM/cache\"\n",
    "\n",
    "def inspect_output(hf_array, megatron_array):\n",
    "    print(f\"hf_array.shape: {hf_array.shape}, megatron_array.shape: {megatron_array.shape}\")\n",
    "    diff = np.abs(hf_array-megatron_array)\n",
    "    min_diff = diff.min()\n",
    "    max_diff = diff.max()\n",
    "    mean_diff = diff.mean()\n",
    "    print(f\"min_diff: {min_diff}, max_diff: {max_diff}, mean_diff: {mean_diff}\")\n",
    "    r_diff = diff/(np.abs(hf_array)+1e-7)\n",
    "    min_r_diff = r_diff.min()\n",
    "    max_r_diff = r_diff.max()\n",
    "    mean_r_diff = r_diff.mean()\n",
    "    print(f\"min_r_diff: {min_r_diff}, max_r_diff: {max_r_diff}, mean_r_diff: {mean_r_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.63671875, mean_diff: 0.003911066800355911\n",
      "min_r_diff: 0.0, max_r_diff: 35073.8515625, mean_r_diff: 4.621415138244629\n",
      "embedding dropout:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.63671875, mean_diff: 0.003911066800355911\n",
      "min_r_diff: 0.0, max_r_diff: 35073.8515625, mean_r_diff: 4.621415138244629\n"
     ]
    }
   ],
   "source": [
    "embedding_hf = np.load(\"/root/Megatron-LM/cache/hf_model.embed_tokens.npy\")\n",
    "embedding_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.word_embeddings.npy\")\n",
    "embedding_flag = np.allclose(embedding_hf, embedding_megatron, atol=atol, rtol=rtol)\n",
    "print(\"embedding: \", embedding_flag)\n",
    "inspect_output(embedding_hf, embedding_megatron)\n",
    "# embedding dropout\n",
    "embedding_dropout_hf = np.load(\"/root/Megatron-LM/cache/hf_model.dropout.npy\")\n",
    "embedding_dropout_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.embedding_dropout.npy\")\n",
    "dropout_flag = np.allclose(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2), atol=atol, rtol=rtol)\n",
    "print(\"embedding dropout: \", dropout_flag)\n",
    "inspect_output(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 8.875, mean_diff: 0.1206251010298729\n",
      "min_r_diff: 0.0, max_r_diff: 858892.5625, mean_r_diff: 18.869600296020508\n"
     ]
    }
   ],
   "source": [
    "if not QKV_LINEAR_FUSION:\n",
    "    rmsnorm_output_hf = np.load(f\"{cache_dir}/hf_model.layers.0.input_layernorm.npy\")\n",
    "    rmsnorm_output_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.input_layernorm.npy\").reshape(1, 128, -1)\n",
    "    inspect_output(rmsnorm_output_hf, rmsnorm_output_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear q:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 4.546875, mean_diff: 0.22054658830165863\n",
      "min_r_diff: 0.0, max_r_diff: 5859375.0, mean_r_diff: 131.69522094726562\n",
      "linear k:  False\n",
      "hf_array.shape: (1, 128, 1024), megatron_array.shape: (1, 128, 1024)\n",
      "min_diff: 0.0, max_diff: 4.578125, mean_diff: 0.3466815948486328\n",
      "min_r_diff: 0.0, max_r_diff: 16562500.0, mean_r_diff: 377.6874084472656\n",
      "linear v:  False\n",
      "hf_array.shape: (1, 128, 1024), megatron_array.shape: (1, 128, 1024)\n",
      "min_diff: 0.0, max_diff: 0.3662109375, mean_diff: 0.02995196171104908\n",
      "min_r_diff: 0.0, max_r_diff: 493164.0625, mean_r_diff: 66.99677276611328\n"
     ]
    }
   ],
   "source": [
    "# qkv_proj\n",
    "if QKV_LINEAR_FUSION:\n",
    "    qkv_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_qkv_o0.npy\")\n",
    "    q_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "    k_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, 512:640].reshape(1, 128, -1)\n",
    "    v_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, 640:].reshape(1, 128, -1)\n",
    "else:\n",
    "    q_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_q_o0.npy\").reshape(1, 128, -1)\n",
    "    k_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_k_o0.npy\").reshape(1, 128, -1)\n",
    "    v_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_v_o0.npy\").reshape(1, 128, -1)\n",
    "q_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.q_proj.npy\")\n",
    "k_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.k_proj.npy\")\n",
    "v_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.v_proj.npy\")\n",
    "layer_norm_flag = np.allclose(q_megatron, q_hf, atol=atol, rtol=rtol) \n",
    "print(\"linear q: \", layer_norm_flag)\n",
    "inspect_output(q_megatron, q_hf)\n",
    "layer_norm_flag = np.allclose(k_megatron, k_hf, atol=atol, rtol=rtol)\n",
    "print(\"linear k: \", layer_norm_flag)\n",
    "inspect_output(k_megatron, k_hf)\n",
    "layer_norm_flag = np.allclose(v_megatron, v_hf, atol=atol, rtol=rtol)\n",
    "print(\"linear v: \", layer_norm_flag)\n",
    "inspect_output(v_megatron, v_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.669921875, mean_diff: 0.0021808231249451637\n",
      "min_r_diff: 0.0, max_r_diff: 45166.015625, mean_r_diff: 8.764433860778809\n",
      "layer 1:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.5078125, mean_diff: 0.027799639850854874\n",
      "min_r_diff: 0.0, max_r_diff: 791015.625, mean_r_diff: 59.15604782104492\n",
      "layer 2:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 2.171142578125, mean_diff: 0.02867942489683628\n",
      "min_r_diff: 0.0, max_r_diff: 693359.375, mean_r_diff: 54.3079948425293\n",
      "layer 3:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.423828125, mean_diff: 0.03147527202963829\n",
      "min_r_diff: 0.0, max_r_diff: 612792.9375, mean_r_diff: 45.11858367919922\n",
      "layer 4:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.09130859375, mean_diff: 0.04509344324469566\n",
      "min_r_diff: 0.0, max_r_diff: 1147460.875, mean_r_diff: 105.95318603515625\n",
      "layer 5:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 4.158203125, mean_diff: 0.037122294306755066\n",
      "min_r_diff: 0.0, max_r_diff: 966796.875, mean_r_diff: 92.5853042602539\n",
      "layer 6:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 2.3154296875, mean_diff: 0.04191335290670395\n",
      "min_r_diff: 0.0, max_r_diff: 1064453.125, mean_r_diff: 72.09268951416016\n",
      "layer 7:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 2.4765625, mean_diff: 0.034619059413671494\n",
      "min_r_diff: 0.0, max_r_diff: 3085937.5, mean_r_diff: 57.67694854736328\n",
      "layer 8:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.919921875, mean_diff: 0.02510421723127365\n",
      "min_r_diff: 0.0, max_r_diff: 566406.25, mean_r_diff: 21.515321731567383\n",
      "layer 9:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.873291015625, mean_diff: 0.022754043340682983\n",
      "min_r_diff: 0.0, max_r_diff: 454101.5625, mean_r_diff: 39.84880065917969\n",
      "layer 10:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.78564453125, mean_diff: 0.03047974407672882\n",
      "min_r_diff: 0.0, max_r_diff: 598144.5, mean_r_diff: 42.039527893066406\n",
      "layer 11:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.623046875, mean_diff: 0.02964758314192295\n",
      "min_r_diff: 0.0, max_r_diff: 1513671.875, mean_r_diff: 48.0372428894043\n",
      "layer 12:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.64013671875, mean_diff: 0.036755796521902084\n",
      "min_r_diff: 0.0, max_r_diff: 1494140.625, mean_r_diff: 77.74581146240234\n",
      "layer 13:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.03564453125, mean_diff: 0.039698995649814606\n",
      "min_r_diff: 0.0, max_r_diff: 1230468.75, mean_r_diff: 98.54750061035156\n",
      "layer 14:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.5419921875, mean_diff: 0.04950534179806709\n",
      "min_r_diff: 0.0, max_r_diff: 1044921.875, mean_r_diff: 114.25509643554688\n",
      "layer 15:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 1.658203125, mean_diff: 0.0616941936314106\n",
      "min_r_diff: 0.0, max_r_diff: 1816406.25, mean_r_diff: 170.49476623535156\n",
      "layer 16:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 2.921875, mean_diff: 0.045149240642786026\n",
      "min_r_diff: 0.0, max_r_diff: 834960.9375, mean_r_diff: 49.896480560302734\n",
      "layer 17:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 4.9375, mean_diff: 0.04783215373754501\n",
      "min_r_diff: 0.0, max_r_diff: 3046875.0, mean_r_diff: 106.81642150878906\n",
      "layer 18:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 2.625, mean_diff: 0.05353375896811485\n",
      "min_r_diff: 0.0, max_r_diff: 1718750.0, mean_r_diff: 123.33843994140625\n",
      "layer 19:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 5.734375, mean_diff: 0.04528007283806801\n",
      "min_r_diff: 0.0, max_r_diff: 1357421.875, mean_r_diff: 89.41793823242188\n",
      "layer 20:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 5.78125, mean_diff: 0.05068110302090645\n",
      "min_r_diff: 0.0, max_r_diff: 937500.0, mean_r_diff: 114.18098449707031\n",
      "layer 21:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 3.169921875, mean_diff: 0.06536310166120529\n",
      "min_r_diff: 0.0, max_r_diff: 1289062.5, mean_r_diff: 121.08726501464844\n",
      "layer 22:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 4.4970703125, mean_diff: 0.07619057595729828\n",
      "min_r_diff: 0.0, max_r_diff: 1904296.875, mean_r_diff: 146.52964782714844\n",
      "layer 23:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 3.2421875, mean_diff: 0.06978229433298111\n",
      "min_r_diff: 0.0, max_r_diff: 2148437.5, mean_r_diff: 135.1376190185547\n",
      "layer 24:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 4.6796875, mean_diff: 0.06894226372241974\n",
      "min_r_diff: 0.0, max_r_diff: 2050781.25, mean_r_diff: 148.90966796875\n",
      "layer 25:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 4.875, mean_diff: 0.06726418435573578\n",
      "min_r_diff: 0.0, max_r_diff: 2031250.0, mean_r_diff: 159.5074005126953\n",
      "layer 26:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 5.375, mean_diff: 0.07300000637769699\n",
      "min_r_diff: 0.0, max_r_diff: 1591796.875, mean_r_diff: 146.37002563476562\n",
      "layer 27:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 3.21875, mean_diff: 0.0702468603849411\n",
      "min_r_diff: 0.0, max_r_diff: 1494140.625, mean_r_diff: 96.4883041381836\n",
      "layer 28:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 3.0234375, mean_diff: 0.07572785019874573\n",
      "min_r_diff: 0.0, max_r_diff: 1523437.5, mean_r_diff: 106.50391387939453\n",
      "layer 29:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 3.703125, mean_diff: 0.09101391583681107\n",
      "min_r_diff: 0.0, max_r_diff: 2011718.75, mean_r_diff: 154.913818359375\n",
      "layer 30:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 16.21875, mean_diff: 0.11651253700256348\n",
      "min_r_diff: 0.0, max_r_diff: 4121093.75, mean_r_diff: 227.1958770751953\n",
      "layer 31:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 12.4375, mean_diff: 0.16880552470684052\n",
      "min_r_diff: 0.0, max_r_diff: 5468750.0, mean_r_diff: 402.4596862792969\n"
     ]
    }
   ],
   "source": [
    "# attention output\n",
    "for i in range(32):\n",
    "    attention_output_hf = np.load(f\"{cache_dir}/hf_model.layers.{i}.self_attn_o0.npy\")\n",
    "    attention_output_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.{i}.self_attention_o0.npy\").transpose(1,0,2)\n",
    "    print(f\"layer {i}: \", np.allclose(attention_output_hf, attention_output_megatron, atol=atol, rtol=rtol))\n",
    "    inspect_output(attention_output_hf, attention_output_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 184622), megatron_array.shape: (1, 128, 184622)\n",
      "min_diff: 0.0, max_diff: 16.53125, mean_diff: 1.6123559474945068\n",
      "min_r_diff: 0.0, max_r_diff: 23678918.0, mean_r_diff: 13.944173812866211\n",
      "94\n",
      "[  3837  35946  99916   3837  99580 101651   3837  18493  26940  99827\n",
      "   9370  53772  48921  99480  48738   3837  35946 100378   9370 110205\n",
      " 100539  18830  21894  99261  97706 102336  52801 100094   3837  41683\n",
      "  77288  26940  99843 101128   3837  35946  99520  99801 100539 104013\n",
      " 101913 100626 104013 100539 101036  99580   2073 100003  35946 101913\n",
      "  61443  16038 104032 104032  99652  33126  99880 100146 100146   3837\n",
      " 104013   9370 104413 104013 104013  31838 104355 102710 104355 109624\n",
      "   3837 151850 100201 113177 100201 103947   3837  42411  99774 100798\n",
      " 104458   3837  27442  31235   3837  42411  99787  34187  42411   3837\n",
      "  42411 101197 101933   3837]\n",
      "[    11    323 100371    198  65101 100903      8  99165  97639  68536\n",
      " 100320 116137  68536 100261     17  99450     16 102650 100132 100873\n",
      "  77288 103982  99651  99165  35946 101437  33447  37474 100378   9370\n",
      " 101075 100378 109843  21596 100378 101075  37643  99844   2073  18600\n",
      "   3837  77288  32664  46944   3837  18830 106097   3837   2073  53930\n",
      " 109234 100158  80158  35946  80158  99998  99729  99164  99245  99245\n",
      "   9370   3837   3837  77288  46944   5373   3837 104672   3837   3837\n",
      " 104934     16  46944  18493  46944  17340 101201  45181  45181 101108\n",
      "  33108 101194  99639 101108  71268   3837 100423 102677 102463 102463\n",
      " 102677  63109 104934   8997]\n"
     ]
    }
   ],
   "source": [
    "output_hf = np.load(f\"{cache_dir}/hf_lm_head.npy\")\n",
    "output_megatron = np.load(f\"{cache_dir}/megatron_output_layer_o0.npy\").transpose(1,0,2)\n",
    "inspect_output(output_hf, output_megatron)\n",
    "np.allclose(output_hf, output_megatron, atol=atol, rtol=rtol)\n",
    "\n",
    "output_hf_label = output_hf.argmax(axis=-1)\n",
    "output_megatron_label = output_megatron.argmax(axis=-1)\n",
    "print(np.sum(output_hf_label!=output_megatron_label))\n",
    "print(output_hf_label[output_hf_label!=output_megatron_label])\n",
    "print(output_megatron_label[output_hf_label!=output_megatron_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (128, 128), megatron_array.shape: (128, 128)\n",
      "min_diff: 0.0, max_diff: 0.0019521117210388184, mean_diff: 0.0004719264688901603\n",
      "min_r_diff: 0.0, max_r_diff: 0.0038003837689757347, mean_r_diff: 0.000658641045447439\n",
      "hf_array.shape: (128, 128), megatron_array.shape: (128, 128)\n",
      "min_diff: 0.0, max_diff: 0.001952826976776123, mean_diff: 0.0003145454975310713\n",
      "min_r_diff: 0.0, max_r_diff: 0.0038327821530401707, mean_r_diff: 0.0013541332446038723\n"
     ]
    }
   ],
   "source": [
    "# rotary embedding\n",
    "cos_hf = np.load(f\"{cache_dir}/hf_model.layers.1.self_attn.rotary_emb_o0.npy\")\n",
    "sin_hf = np.load(f\"{cache_dir}/hf_model.layers.1.self_attn.rotary_emb_o1.npy\")\n",
    "cos_megatron = np.cos(np.load(f\"{cache_dir}/megatron_rotary_pos_emb.npy\")).reshape(128, 128)\n",
    "sin_megatron = np.sin(np.load(f\"{cache_dir}/megatron_rotary_pos_emb.npy\")).reshape(128, 128)\n",
    "inspect_output(cos_hf, cos_megatron)\n",
    "inspect_output(sin_hf, sin_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_megatron = np.load(f'{cache_dir}/megatron_rotary_pos_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.from_numpy(q_hf).cuda().reshape(1, 128, 32, -1).transpose(1, 2)\n",
    "k = torch.from_numpy(k_hf).cuda().reshape(1, 128, 8, -1).transpose(1, 2)\n",
    "v = torch.from_numpy(v_hf).cuda().reshape(1, 128, 8, -1).transpose(1, 2)\n",
    "cos, sin = torch.from_numpy(np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.rotary_emb_o0.npy\")), torch.from_numpy(np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.rotary_emb_o1.npy\"))\n",
    "q, k = apply_rotary_pos_emb(q, k, cos.cuda(), sin.cuda(), position_ids=torch.arange(128, device=\"cuda\").unsqueeze(0))\n",
    "k = k[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "v = v[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "\n",
    "output = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=True).permute(0, 2, 1, 3).reshape(1, 128, -1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformer_engine.pytorch as te\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG \"] = \":4096:8\"\n",
    "\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Emu3RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Emu3RMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class TorchRMSNorm(nn.Module):\n",
    "    def __init__(self, in_features, zero_centered_gamma, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.in_features = in_features\n",
    "        self.zero_centered_gamma = zero_centered_gamma\n",
    "\n",
    "        initial_value = torch.ones(in_features) if zero_centered_gamma else torch.zeros(in_features)\n",
    "        self.weight = nn.Parameter(initial_value)\n",
    "        self.register_parameter(\"weight\", self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x2 = torch.sum(x.float() ** 2, dim=-1, keepdim=True)\n",
    "        d_x = self.in_features\n",
    "\n",
    "        rms_x2 = norm_x2 / d_x + self.eps\n",
    "        r_rms_x = rms_x2 ** (-1.0 / 2)\n",
    "        x_normed = x * r_rms_x\n",
    "\n",
    "        w = self.weight.float()\n",
    "        if self.zero_centered_gamma:\n",
    "            w = 1 + w\n",
    "        return (w * x_normed).to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082487804349512\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013991205487400293\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00021006364841014147\n",
      "min_r_diff: 0.0, max_r_diff: 0.013888886198401451, mean_r_diff: 0.0016162245301529765\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013990971492603421\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.0002690280962269753\n",
      "min_r_diff: 0.0, max_r_diff: 0.014084496535360813, mean_r_diff: 0.0020730604883283377\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n",
      "min_r_diff: 0.0, max_r_diff: 0.006097545847296715, mean_r_diff: 2.326020442922072e-08\n"
     ]
    }
   ],
   "source": [
    "# case I, float32 input\n",
    "\n",
    "input = np.load(f\"{cache_dir}/hf_model.dropout.npy\")\n",
    "input = torch.from_numpy(input).cuda().bfloat16()\n",
    "\n",
    "weight = np.load(f\"{cache_dir}/megatron_rmsnorm_weight.npy\")\n",
    "weight = torch.from_numpy(weight).cuda().bfloat16()\n",
    "\n",
    "hf_rms_norm = Emu3RMSNorm(4096, eps=1e-5)\n",
    "hf_rms_norm.weight.requires_grad = False\n",
    "\n",
    "te_rms_norm = te.RMSNorm(4096, eps=1e-5, params_dtype=torch.bfloat16)\n",
    "te_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm = torch.nn.RMSNorm(4096, eps=1e-5)\n",
    "torch_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm_2 = TorchRMSNorm(4096, zero_centered_gamma=False, eps=1e-5)\n",
    "torch_rms_norm_2.weight.requires_grad = False\n",
    "\n",
    "hf_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "te_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm_2.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_hf = hf_rms_norm(input).cpu().float().numpy()\n",
    "    output_te = te_rms_norm(input).cpu().float().numpy()\n",
    "    output_torch = torch_rms_norm(input).cpu().float().numpy()\n",
    "    output_torch_2 = torch_rms_norm_2(input).cpu().float().numpy()\n",
    "inspect_output(output_hf, output_te)\n",
    "inspect_output(output_hf, output_torch)\n",
    "inspect_output(output_hf, output_torch_2)\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qkv Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input = hf_rms_norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00014284173084888607\n",
      "min_r_diff: 0.0, max_r_diff: 29.493080139160156, mean_r_diff: 0.002411804161965847\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00014285619545262307\n",
      "min_r_diff: 0.0, max_r_diff: 29.493080139160156, mean_r_diff: 0.002412014175206423\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0078125, mean_diff: 3.199484126525931e-07\n",
      "min_r_diff: 0.0, max_r_diff: 0.0712229534983635, mean_r_diff: 3.683098839246668e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "te_weight = np.load(f'{cache_dir}/megatron_qkv_linear_weight.npy')\n",
    "hf_q_weight = np.load(f'{cache_dir}/hf_q_linear_weight.npy')\n",
    "\n",
    "\n",
    "te_linear = te.Linear(6144, 4096, params_dtype=torch.bfloat16, bias=False, device=\"cuda\")\n",
    "te_linear.weight.requires_grad=False\n",
    "te_linear.weight = torch.nn.Parameter(torch.from_numpy(te_weight).cuda().bfloat16())\n",
    "\n",
    "hf_q_linear = torch.nn.Linear(4096, 4096, bias=False, dtype=torch.bfloat16)\n",
    "hf_q_linear.weight.requires_grad=False\n",
    "hf_q_linear.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda().bfloat16())\n",
    "\n",
    "hf_q_linear_fp32 = torch.nn.Linear(4096, 4096, bias=False)\n",
    "hf_q_linear_fp32.weight.requires_grad=False\n",
    "hf_q_linear_fp32.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda())\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_te = te_linear(input).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "    q_hf = hf_q_linear(input)\n",
    "    q_hf_fp32 = hf_q_linear_fp32(input.float()).bfloat16()\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf.cpu().float().numpy())\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf_fp32.cpu().float().numpy()) \n",
    "inspect_output(q_hf.cpu().float().numpy(), q_hf_fp32.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1024, 4096), megatron_array.shape: (1024, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "te_q_linear_weight = np.load(f'{cache_dir}/megatron_v_linear_weight.npy')\n",
    "hf_q_linear_weight = np.load(f'{cache_dir}/hf_v_linear_weight.npy')\n",
    "inspect_output(te_q_linear_weight, hf_q_linear_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (4096, 4096), megatron_array.shape: (4096, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "te_q_linear_weight = np.load(f'{cache_dir}/megatron_qkv_linear_weight.npy').reshape(8, -1, 4096)[:, :512, :].reshape(4096, 4096)\n",
    "hf_q_linear_weight = np.load(f'{cache_dir}/hf_q_linear_weight.npy')\n",
    "inspect_output(te_q_linear_weight, hf_q_linear_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n",
    "te_linear_q = te.Linear(4096, 4096, params_dtype=torch.bfloat16, bias=False, device=\"cuda\")\n",
    "te_linear_q.weight.requires_grad=False\n",
    "te_linear_q.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda().bfloat16())\n",
    "with torch.no_grad():\n",
    "    q_te = te_linear_q(input)\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.0002010889584198594\n",
      "min_r_diff: 0.0, max_r_diff: 118.507568359375, mean_r_diff: 0.004296050872653723\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n",
    "torch_linear_qkv = torch.nn.Linear(6144, 4096, bias=False, dtype=torch.bfloat16)\n",
    "torch_linear_qkv.weight.requires_grad=False\n",
    "torch_linear_qkv.weight = torch.nn.Parameter(torch.from_numpy(te_weight).cuda().bfloat16())\n",
    "with torch.no_grad():\n",
    "    q_torch = torch_linear_qkv(input).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "inspect_output(q_torch.cpu().float().numpy(), q_hf.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NVTE_FLASH_ATTN\"] = \"0\"\n",
    "os.environ[\"NVTE_FUSED_ATTN\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "from transformer_engine.pytorch.utils import attention_mask_func\n",
    "\n",
    "class TorchScaledMaskedSoftmax(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, inp: torch.Tensor, mask: torch.Tensor, scale: Optional[float] = None\n",
    "    ) -> torch.Tensor:\n",
    "        dtype = inp.dtype\n",
    "        inp = inp.float()\n",
    "\n",
    "        if scale is not None:\n",
    "            inp = inp * scale\n",
    "        mask_output = attention_mask_func(inp, mask) if mask is not None else inp\n",
    "\n",
    "        probs = torch.nn.Softmax(dim=-1)(mask_output)\n",
    "        probs = probs.to(dtype)\n",
    "        return probs\n",
    "\n",
    "class TorchDotProductAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kv_channels: int,\n",
    "        attention_dropout: float = 0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_factor = math.sqrt(kv_channels)\n",
    "        self.scale_mask_softmax = TorchScaledMaskedSoftmax()\n",
    "        self.attention_dropout = torch.nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_layer: torch.Tensor,\n",
    "        key_layer: torch.Tensor,\n",
    "        value_layer: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seqlen = query_layer.shape[1], query_layer.shape[0]\n",
    "\n",
    "        # [b, np, sq, sk]\n",
    "        output_size = (\n",
    "            query_layer.size(1),\n",
    "            query_layer.size(2),\n",
    "            query_layer.size(0),\n",
    "            key_layer.size(0),\n",
    "        )\n",
    "\n",
    "        # [sq, b, np, hn] -> [sq, b * np, hn]\n",
    "        query_layer = query_layer.reshape(output_size[2], output_size[0] * output_size[1], -1)\n",
    "        # [sk, b, np, hn] -> [sk, b * np, hn]\n",
    "        key_layer = key_layer.reshape(output_size[3], output_size[0] * output_size[1], -1)\n",
    "\n",
    "        # preallocting result tensor: [b * np, sq, sk]\n",
    "        matmul_result = torch.empty(\n",
    "            output_size[0] * output_size[1],\n",
    "            output_size[2],\n",
    "            output_size[3],\n",
    "            dtype=query_layer.dtype,\n",
    "            device=torch.cuda.current_device(),\n",
    "        )\n",
    "\n",
    "        # Raw attention scores. [b * np, sq, sk]\n",
    "        matmul_result = torch.baddbmm(\n",
    "            matmul_result,\n",
    "            query_layer.transpose(0, 1),  # [b * np, sq, hn]\n",
    "            key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n",
    "            beta=0.0,\n",
    "            alpha=(1.0 / self.norm_factor),\n",
    "        )\n",
    "\n",
    "        # change view to [b, np, sq, sk]\n",
    "        attention_scores = matmul_result.view(*output_size)\n",
    "\n",
    "        # attention scores and attention mask [b, np, sq, sk]\n",
    "        attention_probs = self.scale_mask_softmax(attention_scores, attention_mask)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        # value_layer -> context layer.\n",
    "        # [sk, b, np, hn] --> [b, np, sq, hn]\n",
    "        output_size = (\n",
    "            value_layer.size(1),\n",
    "            value_layer.size(2),\n",
    "            query_layer.size(0),\n",
    "            value_layer.size(3),\n",
    "        )\n",
    "\n",
    "        # change view [sk, b * np, hn]\n",
    "        value_layer = value_layer.reshape(value_layer.size(0), output_size[0] * output_size[1], -1)\n",
    "\n",
    "        # change view [b * np, sq, sk]\n",
    "        attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "        \n",
    "\n",
    "        # matmul: [b * np, sq, hn]\n",
    "        context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))\n",
    "\n",
    "        # change view [b, np, sq, hn]\n",
    "        context_layer = context_layer.view(*output_size)\n",
    "\n",
    "        # [b, np, sq, hn] --> [sq, b, np, hn]\n",
    "        context_layer = context_layer.permute(2, 0, 1, 3).contiguous()\n",
    "\n",
    "        # [sq, b, np, hn] --> [sq, b, hp]\n",
    "        context_layer = context_layer.view(seqlen, batch_size, -1)\n",
    "\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.214874267578125, mean_diff: 0.00832381658256054\n",
      "min_r_diff: 0.0, max_r_diff: 79489.46875, mean_r_diff: 7.876452922821045\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0009765625, mean_diff: 7.922864824649878e-06\n",
      "min_r_diff: 0.0, max_r_diff: 91.75723266601562, mean_r_diff: 0.009910470806062222\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.214874267578125, mean_diff: 0.008323675021529198\n",
      "min_r_diff: 0.0, max_r_diff: 80899.7109375, mean_r_diff: 8.07060718536377\n"
     ]
    }
   ],
   "source": [
    "q = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o0.npy\")).cuda().bfloat16()\n",
    "k = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o1.npy\")).cuda().bfloat16()\n",
    "v = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o2.npy\")).cuda().bfloat16()\n",
    "\n",
    "attention_mask_megatron = torch.triu(torch.ones((128, 128)), diagonal=1).bool().cuda()\n",
    "attention_mask_hf = torch.triu(torch.ones((128, 128)), diagonal=1).bool()\n",
    "attention_mask_hf = attention_mask_hf.unsqueeze(0).unsqueeze(0)\n",
    "attention_mask_hf = attention_mask_hf.to(device=\"cuda\")\n",
    "attention_mask_hf = torch.zeros_like(attention_mask_hf, dtype=torch.float32).masked_fill(attention_mask_megatron, -10000).bfloat16()\n",
    "\n",
    "attn_te = te.DotProductAttention(num_attention_heads=32, num_gqa_groups=8, kv_channels=128, attention_dropout=0.0, attn_mask_type=\"causal\")\n",
    "output_te = attn_te(q, k, v).transpose(1, 0).cpu().float().numpy()\n",
    "attn_torch = TorchDotProductAttention(kv_channels=128, attention_dropout=0.0)\n",
    "k2 = k[:, :, :, None, :].expand(-1, -1, -1, 4, -1).reshape(128, 1, 32, 128)\n",
    "v2 = v[:, :, :, None, :].expand(-1, -1, -1, 4, -1).reshape(128, 1, 32, 128)\n",
    "output_torch2 = attn_torch(q, k2, v2, attention_mask=attention_mask_megatron).transpose(1, 0).cpu().float().numpy()\n",
    "\n",
    "\n",
    "# S B H D -> B H S D\n",
    "q = q.permute(1, 2, 0, 3)\n",
    "k = k.permute(1, 2, 0, 3)\n",
    "v = v.permute(1, 2, 0, 3)\n",
    "k = k[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "v = v[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "# Only enable flash attention backend\n",
    "with sdpa_kernel([SDPBackend.MATH]):\n",
    "    output_torch = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=~attention_mask_megatron, dropout_p=0.0).permute(1, 0, 2, 3).reshape(1, 128, -1).cpu().float().numpy()\n",
    "# results = torch.empty(\n",
    "#     q.size(0)*q.size(1),\n",
    "#     q.size(2),\n",
    "#     k.size(2),\n",
    "#     dtype=q.dtype,\n",
    "#     device=q.device,\n",
    "# )\n",
    "# attn_weights = torch.baddbmm(results, q.reshape(q.size(0)*q.size(1), q.size(2), -1), k.transpose(2, 3).reshape(k.size(0)*k.size(1), k.size(2), -1), beta=0.0, alpha=1.0 / math.sqrt(128))\n",
    "# attn_weights = attn_weights.view(q.size(0), q.size(1), q.size(2), k.size(2))\n",
    "# attn_weights = attn_weights.masked_fill(attention_mask_megatron, -10000)\n",
    "\n",
    "\n",
    "# # upcast attention to fp32\n",
    "# attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "# output_torch = torch.matmul(attn_weights, v)\n",
    "# output_torch = output_torch.permute(0, 2, 1, 3).reshape(1, 128, -1).cpu().float().numpy()\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch2)\n",
    "inspect_output(output_torch, output_torch2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082487804349512\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013991205487400293\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.011625289916992188, mean_diff: 0.0002484717406332493\n",
      "min_r_diff: 0.0, max_r_diff: 0.0076371352188289165, mean_r_diff: 0.0019240325782448053\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013990971492603421\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0077877044677734375, mean_diff: 0.00018516556883696467\n",
      "min_r_diff: 0.0, max_r_diff: 0.004377623554319143, mean_r_diff: 0.0014343515504151583\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n",
      "min_r_diff: 0.0, max_r_diff: 0.006097545847296715, mean_r_diff: 2.326020442922072e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Megatron-LM/.pixi/envs/default/lib/python3.10/site-packages/transformer_engine/pytorch/module/rmsnorm.py:193: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at ../torch/csrc/autograd/init.cpp:787.)\n",
      "  self.activation_dtype = torch.get_autocast_gpu_dtype()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# case I, float32 input\n",
    "\n",
    "input = np.load(f\"{cache_dir}/hf_model.dropout.npy\")\n",
    "input = torch.from_numpy(input).cuda().bfloat16()\n",
    "\n",
    "weight = np.load(f\"{cache_dir}/megatron_rmsnorm_weight.npy\")\n",
    "weight = torch.from_numpy(weight).cuda().bfloat16()\n",
    "\n",
    "hf_rms_norm = Emu3RMSNorm(4096, eps=1e-5)\n",
    "hf_rms_norm.weight.requires_grad = False\n",
    "\n",
    "te_rms_norm = te.RMSNorm(4096, eps=1e-5)\n",
    "te_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm = torch.nn.RMSNorm(4096, eps=1e-5)\n",
    "torch_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm_2 = TorchRMSNorm(4096, zero_centered_gamma=False, eps=1e-5)\n",
    "torch_rms_norm_2.weight.requires_grad = False\n",
    "\n",
    "hf_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "te_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm_2.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        output_hf = hf_rms_norm(input).cpu().float().numpy()\n",
    "        output_te = te_rms_norm(input).cpu().float().numpy()\n",
    "        output_torch = torch_rms_norm(input).cpu().float().numpy()\n",
    "        output_torch_2 = torch_rms_norm_2(input).cpu().float().numpy()\n",
    "inspect_output(output_hf, output_te)\n",
    "inspect_output(output_hf, output_torch)\n",
    "inspect_output(output_hf, output_torch_2)\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n",
      "min_r_diff: 0.0, max_r_diff: 0.006134953815490007, mean_r_diff: 2.340290450320026e-08\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013987725833430886\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0077877044677734375, mean_diff: 0.0001851654815254733\n",
      "min_r_diff: 0.0, max_r_diff: 0.004377623554319143, mean_r_diff: 0.0014343486400321126\n"
     ]
    }
   ],
   "source": [
    "def fp32_rms_norm(x, weight):\n",
    "    x = x.to(torch.float32)\n",
    "    variance = x.pow(2).mean(-1, keepdim=True)\n",
    "    weight = weight.to(torch.float32)\n",
    "    x = x * torch.rsqrt(variance + 1e-5)\n",
    "    return (weight * x).bfloat16()\n",
    "output_fp32 = fp32_rms_norm(input, weight).cpu().float().numpy()\n",
    "inspect_output(output_fp32, output_te)\n",
    "inspect_output(output_fp32, output_hf)\n",
    "inspect_output(output_fp32, output_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
