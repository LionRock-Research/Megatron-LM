{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtype = 'bfloat16'\n",
    "ATOL = {\n",
    "    'float32': 1e-3,\n",
    "    'bfloat16': 5e-2,\n",
    "    'float16': 5e-2,\n",
    "}\n",
    "RTOL = {\n",
    "    'float32': 1.3e-6,\n",
    "    'bfloat16': 1e-2,\n",
    "    'float16': 1e-2,\n",
    "}\n",
    "atol = ATOL[Dtype]\n",
    "rtol = RTOL[Dtype]\n",
    "QKV_LINEAR_FUSION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/root/Megatron-LM/cache\"\n",
    "\n",
    "def inspect_output(hf_array, megatron_array):\n",
    "    print(f\"hf_array.shape: {hf_array.shape}, megatron_array.shape: {megatron_array.shape}\")\n",
    "    diff = np.abs(hf_array-megatron_array)\n",
    "    min_diff = diff.min()\n",
    "    max_diff = diff.max()\n",
    "    mean_diff = diff.mean()\n",
    "    print(f\"min_diff: {min_diff}, max_diff: {max_diff}, mean_diff: {mean_diff}\")\n",
    "    r_diff = diff/(np.abs(hf_array)+1e-7)\n",
    "    min_r_diff = r_diff.min()\n",
    "    max_r_diff = r_diff.max()\n",
    "    mean_r_diff = r_diff.mean()\n",
    "    print(f\"min_r_diff: {min_r_diff}, max_r_diff: {max_r_diff}, mean_r_diff: {mean_r_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n",
      "embedding dropout:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "embedding_hf = np.load(\"/root/Megatron-LM/cache/hf_model.embed_tokens.npy\")\n",
    "embedding_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.word_embeddings.npy\")\n",
    "embedding_flag = np.allclose(embedding_hf, embedding_megatron, atol=atol, rtol=rtol)\n",
    "print(\"embedding: \", embedding_flag)\n",
    "inspect_output(embedding_hf, embedding_megatron)\n",
    "# embedding dropout\n",
    "embedding_dropout_hf = np.load(\"/root/Megatron-LM/cache/hf_model.dropout.npy\")\n",
    "embedding_dropout_megatron = np.load(\"/root/Megatron-LM/cache/megatron_embedding.embedding_dropout.npy\")\n",
    "dropout_flag = np.allclose(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2), atol=atol, rtol=rtol)\n",
    "print(\"embedding dropout: \", dropout_flag)\n",
    "inspect_output(embedding_dropout_hf, embedding_dropout_megatron.transpose(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not QKV_LINEAR_FUSION:\n",
    "    rmsnorm_output_hf = np.load(f\"{cache_dir}/hf_model.layers.0.input_layernorm.npy\")\n",
    "    rmsnorm_output_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.input_layernorm.npy\").reshape(1, 128, -1)\n",
    "    inspect_output(rmsnorm_output_hf, rmsnorm_output_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear q:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.000721507822163403\n",
      "min_r_diff: 0.0, max_r_diff: 420.6923522949219, mean_r_diff: 0.010125597007572651\n",
      "linear k:  True\n",
      "hf_array.shape: (1, 128, 1024), megatron_array.shape: (1, 128, 1024)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.0011264100903645158\n",
      "min_r_diff: 0.0, max_r_diff: 116.12904357910156, mean_r_diff: 0.011339498683810234\n",
      "linear v:  True\n",
      "hf_array.shape: (1, 128, 1024), megatron_array.shape: (1, 128, 1024)\n",
      "min_diff: 0.0, max_diff: 0.00390625, mean_diff: 7.616392394993454e-05\n",
      "min_r_diff: 0.0, max_r_diff: 275.34375, mean_r_diff: 0.030313245952129364\n"
     ]
    }
   ],
   "source": [
    "# qkv_proj\n",
    "if QKV_LINEAR_FUSION:\n",
    "    qkv_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_qkv_o0.npy\")\n",
    "    q_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "    k_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, 512:640].reshape(1, 128, -1)\n",
    "    v_megatron = qkv_megatron.transpose(1,0,2).reshape(1, 128, 8, -1)[:, :, :, 640:].reshape(1, 128, -1)\n",
    "else:\n",
    "    q_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_q_o0.npy\").reshape(1, 128, -1)\n",
    "    k_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_k_o0.npy\").reshape(1, 128, -1)\n",
    "    v_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.linear_v_o0.npy\").reshape(1, 128, -1)\n",
    "q_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.q_proj.npy\")\n",
    "k_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.k_proj.npy\")\n",
    "v_hf = np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.v_proj.npy\")\n",
    "layer_norm_flag = np.allclose(q_megatron, q_hf, atol=atol, rtol=rtol) \n",
    "print(\"linear q: \", layer_norm_flag)\n",
    "inspect_output(q_megatron, q_hf)\n",
    "layer_norm_flag = np.allclose(k_megatron, k_hf, atol=atol, rtol=rtol)\n",
    "print(\"linear k: \", layer_norm_flag)\n",
    "inspect_output(k_megatron, k_hf)\n",
    "layer_norm_flag = np.allclose(v_megatron, v_hf, atol=atol, rtol=rtol)\n",
    "print(\"linear v: \", layer_norm_flag)\n",
    "inspect_output(v_megatron, v_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.00390625, mean_diff: 1.3048614164290484e-05\n",
      "min_r_diff: 0.0, max_r_diff: 610.3515625, mean_r_diff: 0.08150769770145416\n",
      "layer 1:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0234375, mean_diff: 0.00023041712120175362\n",
      "min_r_diff: 0.0, max_r_diff: 7476.806640625, mean_r_diff: 0.6095441579818726\n",
      "layer 2:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.0002974425442516804\n",
      "min_r_diff: 0.0, max_r_diff: 14038.0859375, mean_r_diff: 0.8078778386116028\n",
      "layer 3:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.0004942230298183858\n",
      "min_r_diff: 0.0, max_r_diff: 26855.46875, mean_r_diff: 1.3757848739624023\n",
      "layer 4:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.01953125, mean_diff: 0.0006292047328315675\n",
      "min_r_diff: 0.0, max_r_diff: 21972.65625, mean_r_diff: 2.2397286891937256\n",
      "layer 5:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0625, mean_diff: 0.0005527112516574562\n",
      "min_r_diff: 0.0, max_r_diff: 25634.765625, mean_r_diff: 2.2407164573669434\n",
      "layer 6:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.046875, mean_diff: 0.0006309066084213555\n",
      "min_r_diff: 0.0, max_r_diff: 26702.880859375, mean_r_diff: 1.8093159198760986\n",
      "layer 7:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.046875, mean_diff: 0.0005468781455419958\n",
      "min_r_diff: 0.0, max_r_diff: 29296.875, mean_r_diff: 1.5016038417816162\n",
      "layer 8:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0703125, mean_diff: 0.0004151542379986495\n",
      "min_r_diff: 0.0, max_r_diff: 13732.91015625, mean_r_diff: 0.580342710018158\n",
      "layer 9:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.025390625, mean_diff: 0.0004921657382510602\n",
      "min_r_diff: 0.0, max_r_diff: 11596.6796875, mean_r_diff: 1.0744378566741943\n",
      "layer 10:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.01953125, mean_diff: 0.0005438952357508242\n",
      "min_r_diff: 0.0, max_r_diff: 18310.546875, mean_r_diff: 1.2418923377990723\n",
      "layer 11:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.009765625, mean_diff: 0.00046416991972364485\n",
      "min_r_diff: 0.0, max_r_diff: 22583.0078125, mean_r_diff: 1.038088083267212\n",
      "layer 12:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.04296875, mean_diff: 0.0006252086604945362\n",
      "min_r_diff: 0.0, max_r_diff: 27465.8203125, mean_r_diff: 1.8847782611846924\n",
      "layer 13:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.02734375, mean_diff: 0.0008468097657896578\n",
      "min_r_diff: 0.0, max_r_diff: 43945.3125, mean_r_diff: 2.7411961555480957\n",
      "layer 14:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0234375, mean_diff: 0.0008698511519469321\n",
      "min_r_diff: 0.0, max_r_diff: 54931.640625, mean_r_diff: 3.429464817047119\n",
      "layer 15:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.046875, mean_diff: 0.0009408140904270113\n",
      "min_r_diff: 0.0, max_r_diff: 35400.390625, mean_r_diff: 3.4641835689544678\n",
      "layer 16:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.05078125, mean_diff: 0.0007420605979859829\n",
      "min_r_diff: 0.0, max_r_diff: 36621.09375, mean_r_diff: 1.7383357286453247\n",
      "layer 17:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.046875, mean_diff: 0.0009973117848858237\n",
      "min_r_diff: 0.0, max_r_diff: 140380.859375, mean_r_diff: 3.5215721130371094\n",
      "layer 18:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.05859375, mean_diff: 0.0009657487389631569\n",
      "min_r_diff: 0.0, max_r_diff: 42114.2578125, mean_r_diff: 3.035285711288452\n",
      "layer 19:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.091796875, mean_diff: 0.0006602142821066082\n",
      "min_r_diff: 0.0, max_r_diff: 24414.0625, mean_r_diff: 2.0717530250549316\n",
      "layer 20:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0595703125, mean_diff: 0.0007607433362863958\n",
      "min_r_diff: 0.0, max_r_diff: 32043.45703125, mean_r_diff: 2.766706705093384\n",
      "layer 21:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.1015625, mean_diff: 0.001120394910685718\n",
      "min_r_diff: 0.0, max_r_diff: 39062.5, mean_r_diff: 3.160893201828003\n",
      "layer 22:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0859375, mean_diff: 0.001207255176268518\n",
      "min_r_diff: 0.0, max_r_diff: 46386.71875, mean_r_diff: 3.4688618183135986\n",
      "layer 23:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.140625, mean_diff: 0.0009572624112479389\n",
      "min_r_diff: 0.0, max_r_diff: 40283.203125, mean_r_diff: 2.661206007003784\n",
      "layer 24:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0625, mean_diff: 0.0008625058690086007\n",
      "min_r_diff: 0.0, max_r_diff: 56152.34375, mean_r_diff: 2.7699739933013916\n",
      "layer 25:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.21875, mean_diff: 0.0008175796829164028\n",
      "min_r_diff: 0.0, max_r_diff: 31738.28125, mean_r_diff: 3.412367820739746\n",
      "layer 26:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.046875, mean_diff: 0.0009096692083403468\n",
      "min_r_diff: 0.0, max_r_diff: 31738.28125, mean_r_diff: 2.6481106281280518\n",
      "layer 27:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.234375, mean_diff: 0.0008924626163206995\n",
      "min_r_diff: 0.0, max_r_diff: 36621.09375, mean_r_diff: 2.2973902225494385\n",
      "layer 28:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.078125, mean_diff: 0.0010078290943056345\n",
      "min_r_diff: 0.0, max_r_diff: 33874.51171875, mean_r_diff: 2.821547269821167\n",
      "layer 29:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.060546875, mean_diff: 0.0014077393570914865\n",
      "min_r_diff: 0.0, max_r_diff: 56152.34375, mean_r_diff: 4.809173107147217\n",
      "layer 30:  True\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.0019484416116029024\n",
      "min_r_diff: 0.0, max_r_diff: 80566.40625, mean_r_diff: 6.747208118438721\n",
      "layer 31:  False\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.09375, mean_diff: 0.0025058614555746317\n",
      "min_r_diff: 0.0, max_r_diff: 104980.46875, mean_r_diff: 9.306963920593262\n"
     ]
    }
   ],
   "source": [
    "# attention output\n",
    "for i in range(32):\n",
    "    attention_output_hf = np.load(f\"{cache_dir}/hf_model.layers.{i}.self_attn_o0.npy\")\n",
    "    attention_output_megatron = np.load(f\"{cache_dir}/megatron_decoder.layers.{i}.self_attention_o0.npy\").transpose(1,0,2)\n",
    "    print(f\"layer {i}: \", np.allclose(attention_output_hf, attention_output_megatron, atol=atol, rtol=rtol))\n",
    "    inspect_output(attention_output_hf, attention_output_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 184622), megatron_array.shape: (1, 128, 184622)\n",
      "min_diff: 0.0, max_diff: 0.2265625, mean_diff: 0.010856513865292072\n",
      "min_r_diff: 0.0, max_r_diff: 23336.587890625, mean_r_diff: 0.08597517013549805\n",
      "1\n",
      "[110205]\n",
      "[100873]\n"
     ]
    }
   ],
   "source": [
    "output_hf = np.load(f\"{cache_dir}/hf_lm_head.npy\")\n",
    "output_megatron = np.load(f\"{cache_dir}/megatron_output_layer_o0.npy\").transpose(1,0,2)\n",
    "inspect_output(output_hf, output_megatron)\n",
    "np.allclose(output_hf, output_megatron, atol=atol, rtol=rtol)\n",
    "\n",
    "output_hf_label = output_hf.argmax(axis=-1)\n",
    "output_megatron_label = output_megatron.argmax(axis=-1)\n",
    "print(np.sum(output_hf_label!=output_megatron_label))\n",
    "print(output_hf_label[output_hf_label!=output_megatron_label])\n",
    "print(output_megatron_label[output_hf_label!=output_megatron_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (128, 128), megatron_array.shape: (128, 128)\n",
      "min_diff: 0.0, max_diff: 0.0019521117210388184, mean_diff: 0.0004719264688901603\n",
      "min_r_diff: 0.0, max_r_diff: 0.0038003837689757347, mean_r_diff: 0.000658641045447439\n",
      "hf_array.shape: (128, 128), megatron_array.shape: (128, 128)\n",
      "min_diff: 0.0, max_diff: 0.001952826976776123, mean_diff: 0.0003145454975310713\n",
      "min_r_diff: 0.0, max_r_diff: 0.0038327821530401707, mean_r_diff: 0.0013541332446038723\n"
     ]
    }
   ],
   "source": [
    "# rotary embedding\n",
    "cos_hf = np.load(f\"{cache_dir}/hf_model.layers.1.self_attn.rotary_emb_o0.npy\")\n",
    "sin_hf = np.load(f\"{cache_dir}/hf_model.layers.1.self_attn.rotary_emb_o1.npy\")\n",
    "cos_megatron = np.cos(np.load(f\"{cache_dir}/megatron_rotary_pos_emb.npy\")).reshape(128, 128)\n",
    "sin_megatron = np.sin(np.load(f\"{cache_dir}/megatron_rotary_pos_emb.npy\")).reshape(128, 128)\n",
    "inspect_output(cos_hf, cos_megatron)\n",
    "inspect_output(sin_hf, sin_megatron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_megatron = np.load(f'{cache_dir}/megatron_rotary_pos_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.from_numpy(q_hf).cuda().reshape(1, 128, 32, -1).transpose(1, 2)\n",
    "k = torch.from_numpy(k_hf).cuda().reshape(1, 128, 8, -1).transpose(1, 2)\n",
    "v = torch.from_numpy(v_hf).cuda().reshape(1, 128, 8, -1).transpose(1, 2)\n",
    "cos, sin = torch.from_numpy(np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.rotary_emb_o0.npy\")), torch.from_numpy(np.load(f\"{cache_dir}/hf_model.layers.0.self_attn.rotary_emb_o1.npy\"))\n",
    "q, k = apply_rotary_pos_emb(q, k, cos.cuda(), sin.cuda(), position_ids=torch.arange(128, device=\"cuda\").unsqueeze(0))\n",
    "k = k[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "v = v[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "\n",
    "output = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=True).permute(0, 2, 1, 3).reshape(1, 128, -1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformer_engine.pytorch as te\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG \"] = \":4096:8\"\n",
    "\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Emu3RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Emu3RMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class TorchRMSNorm(nn.Module):\n",
    "    def __init__(self, in_features, zero_centered_gamma, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.in_features = in_features\n",
    "        self.zero_centered_gamma = zero_centered_gamma\n",
    "\n",
    "        initial_value = torch.ones(in_features) if zero_centered_gamma else torch.zeros(in_features)\n",
    "        self.weight = nn.Parameter(initial_value)\n",
    "        self.register_parameter(\"weight\", self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x2 = torch.sum(x.float() ** 2, dim=-1, keepdim=True)\n",
    "        d_x = self.in_features\n",
    "\n",
    "        rms_x2 = norm_x2 / d_x + self.eps\n",
    "        r_rms_x = rms_x2 ** (-1.0 / 2)\n",
    "        x_normed = x * r_rms_x\n",
    "\n",
    "        w = self.weight.float()\n",
    "        if self.zero_centered_gamma:\n",
    "            w = 1 + w\n",
    "        return (w * x_normed).to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082487804349512\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013991205487400293\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00021006364841014147\n",
      "min_r_diff: 0.0, max_r_diff: 0.013888886198401451, mean_r_diff: 0.0016162245301529765\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013990971492603421\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.0002690280962269753\n",
      "min_r_diff: 0.0, max_r_diff: 0.014084496535360813, mean_r_diff: 0.0020730604883283377\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n",
      "min_r_diff: 0.0, max_r_diff: 0.006097545847296715, mean_r_diff: 2.326020442922072e-08\n"
     ]
    }
   ],
   "source": [
    "# case I, float32 input\n",
    "\n",
    "input = np.load(f\"{cache_dir}/hf_model.dropout.npy\")\n",
    "input = torch.from_numpy(input).cuda().bfloat16()\n",
    "\n",
    "weight = np.load(f\"{cache_dir}/megatron_rmsnorm_weight.npy\")\n",
    "weight = torch.from_numpy(weight).cuda().bfloat16()\n",
    "\n",
    "hf_rms_norm = Emu3RMSNorm(4096, eps=1e-5)\n",
    "hf_rms_norm.weight.requires_grad = False\n",
    "\n",
    "te_rms_norm = te.RMSNorm(4096, eps=1e-5, params_dtype=torch.bfloat16)\n",
    "te_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm = torch.nn.RMSNorm(4096, eps=1e-5)\n",
    "torch_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm_2 = TorchRMSNorm(4096, zero_centered_gamma=False, eps=1e-5)\n",
    "torch_rms_norm_2.weight.requires_grad = False\n",
    "\n",
    "hf_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "te_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm_2.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_hf = hf_rms_norm(input).cpu().float().numpy()\n",
    "    output_te = te_rms_norm(input).cpu().float().numpy()\n",
    "    output_torch = torch_rms_norm(input).cpu().float().numpy()\n",
    "    output_torch_2 = torch_rms_norm_2(input).cpu().float().numpy()\n",
    "inspect_output(output_hf, output_te)\n",
    "inspect_output(output_hf, output_torch)\n",
    "inspect_output(output_hf, output_torch_2)\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qkv Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input = hf_rms_norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00014284173084888607\n",
      "min_r_diff: 0.0, max_r_diff: 29.493080139160156, mean_r_diff: 0.002411804161965847\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.00014285619545262307\n",
      "min_r_diff: 0.0, max_r_diff: 29.493080139160156, mean_r_diff: 0.002412014175206423\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0078125, mean_diff: 3.199484126525931e-07\n",
      "min_r_diff: 0.0, max_r_diff: 0.0712229534983635, mean_r_diff: 3.683098839246668e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "te_weight = np.load(f'{cache_dir}/megatron_qkv_linear_weight.npy')\n",
    "hf_q_weight = np.load(f'{cache_dir}/hf_q_linear_weight.npy')\n",
    "\n",
    "\n",
    "te_linear = te.Linear(6144, 4096, params_dtype=torch.bfloat16, bias=False, device=\"cuda\")\n",
    "te_linear.weight.requires_grad=False\n",
    "te_linear.weight = torch.nn.Parameter(torch.from_numpy(te_weight).cuda().bfloat16())\n",
    "\n",
    "hf_q_linear = torch.nn.Linear(4096, 4096, bias=False, dtype=torch.bfloat16)\n",
    "hf_q_linear.weight.requires_grad=False\n",
    "hf_q_linear.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda().bfloat16())\n",
    "\n",
    "hf_q_linear_fp32 = torch.nn.Linear(4096, 4096, bias=False)\n",
    "hf_q_linear_fp32.weight.requires_grad=False\n",
    "hf_q_linear_fp32.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda())\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_te = te_linear(input).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "    q_hf = hf_q_linear(input)\n",
    "    q_hf_fp32 = hf_q_linear_fp32(input.float()).bfloat16()\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf.cpu().float().numpy())\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf_fp32.cpu().float().numpy()) \n",
    "inspect_output(q_hf.cpu().float().numpy(), q_hf_fp32.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1024, 4096), megatron_array.shape: (1024, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "te_q_linear_weight = np.load(f'{cache_dir}/megatron_v_linear_weight.npy')\n",
    "hf_q_linear_weight = np.load(f'{cache_dir}/hf_v_linear_weight.npy')\n",
    "inspect_output(te_q_linear_weight, hf_q_linear_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (4096, 4096), megatron_array.shape: (4096, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "te_q_linear_weight = np.load(f'{cache_dir}/megatron_qkv_linear_weight.npy').reshape(8, -1, 4096)[:, :512, :].reshape(4096, 4096)\n",
    "hf_q_linear_weight = np.load(f'{cache_dir}/hf_q_linear_weight.npy')\n",
    "inspect_output(te_q_linear_weight, hf_q_linear_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0, mean_diff: 0.0\n",
      "min_r_diff: 0.0, max_r_diff: 0.0, mean_r_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n",
    "te_linear_q = te.Linear(4096, 4096, params_dtype=torch.bfloat16, bias=False, device=\"cuda\")\n",
    "te_linear_q.weight.requires_grad=False\n",
    "te_linear_q.weight = torch.nn.Parameter(torch.from_numpy(hf_q_weight).cuda().bfloat16())\n",
    "with torch.no_grad():\n",
    "    q_te = te_linear_q(input)\n",
    "inspect_output(q_te.cpu().float().numpy(), q_hf.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.03125, mean_diff: 0.0002010889584198594\n",
      "min_r_diff: 0.0, max_r_diff: 118.507568359375, mean_r_diff: 0.004296050872653723\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n",
    "torch_linear_qkv = torch.nn.Linear(6144, 4096, bias=False, dtype=torch.bfloat16)\n",
    "torch_linear_qkv.weight.requires_grad=False\n",
    "torch_linear_qkv.weight = torch.nn.Parameter(torch.from_numpy(te_weight).cuda().bfloat16())\n",
    "with torch.no_grad():\n",
    "    q_torch = torch_linear_qkv(input).reshape(1, 128, 8, -1)[:, :, :, :512].reshape(1, 128, -1)\n",
    "inspect_output(q_torch.cpu().float().numpy(), q_hf.cpu().float().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NVTE_FLASH_ATTN\"] = \"0\"\n",
    "os.environ[\"NVTE_FUSED_ATTN\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "from transformer_engine.pytorch.utils import attention_mask_func\n",
    "\n",
    "class TorchScaledMaskedSoftmax(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, inp: torch.Tensor, mask: torch.Tensor, scale: Optional[float] = None\n",
    "    ) -> torch.Tensor:\n",
    "        dtype = inp.dtype\n",
    "        inp = inp.float()\n",
    "\n",
    "        if scale is not None:\n",
    "            inp = inp * scale\n",
    "        mask_output = attention_mask_func(inp, mask) if mask is not None else inp\n",
    "\n",
    "        probs = torch.nn.Softmax(dim=-1)(mask_output)\n",
    "        probs = probs.to(dtype)\n",
    "        return probs\n",
    "\n",
    "class TorchDotProductAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kv_channels: int,\n",
    "        attention_dropout: float = 0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_factor = math.sqrt(kv_channels)\n",
    "        self.scale_mask_softmax = TorchScaledMaskedSoftmax()\n",
    "        self.attention_dropout = torch.nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_layer: torch.Tensor,\n",
    "        key_layer: torch.Tensor,\n",
    "        value_layer: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seqlen = query_layer.shape[1], query_layer.shape[0]\n",
    "\n",
    "        # [b, np, sq, sk]\n",
    "        output_size = (\n",
    "            query_layer.size(1),\n",
    "            query_layer.size(2),\n",
    "            query_layer.size(0),\n",
    "            key_layer.size(0),\n",
    "        )\n",
    "\n",
    "        # [sq, b, np, hn] -> [sq, b * np, hn]\n",
    "        query_layer = query_layer.reshape(output_size[2], output_size[0] * output_size[1], -1)\n",
    "        # [sk, b, np, hn] -> [sk, b * np, hn]\n",
    "        key_layer = key_layer.reshape(output_size[3], output_size[0] * output_size[1], -1)\n",
    "\n",
    "        # preallocting result tensor: [b * np, sq, sk]\n",
    "        matmul_result = torch.empty(\n",
    "            output_size[0] * output_size[1],\n",
    "            output_size[2],\n",
    "            output_size[3],\n",
    "            dtype=query_layer.dtype,\n",
    "            device=torch.cuda.current_device(),\n",
    "        )\n",
    "\n",
    "        # Raw attention scores. [b * np, sq, sk]\n",
    "        matmul_result = torch.baddbmm(\n",
    "            matmul_result,\n",
    "            query_layer.transpose(0, 1),  # [b * np, sq, hn]\n",
    "            key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n",
    "            beta=0.0,\n",
    "            alpha=(1.0 / self.norm_factor),\n",
    "        )\n",
    "\n",
    "        # change view to [b, np, sq, sk]\n",
    "        attention_scores = matmul_result.view(*output_size)\n",
    "\n",
    "        # attention scores and attention mask [b, np, sq, sk]\n",
    "        attention_probs = self.scale_mask_softmax(attention_scores, attention_mask)\n",
    "        attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "        # value_layer -> context layer.\n",
    "        # [sk, b, np, hn] --> [b, np, sq, hn]\n",
    "        output_size = (\n",
    "            value_layer.size(1),\n",
    "            value_layer.size(2),\n",
    "            query_layer.size(0),\n",
    "            value_layer.size(3),\n",
    "        )\n",
    "\n",
    "        # change view [sk, b * np, hn]\n",
    "        value_layer = value_layer.reshape(value_layer.size(0), output_size[0] * output_size[1], -1)\n",
    "\n",
    "        # change view [b * np, sq, sk]\n",
    "        attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "        \n",
    "\n",
    "        # matmul: [b * np, sq, hn]\n",
    "        context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))\n",
    "\n",
    "        # change view [b, np, sq, hn]\n",
    "        context_layer = context_layer.view(*output_size)\n",
    "\n",
    "        # [b, np, sq, hn] --> [sq, b, np, hn]\n",
    "        context_layer = context_layer.permute(2, 0, 1, 3).contiguous()\n",
    "\n",
    "        # [sq, b, np, hn] --> [sq, b, hp]\n",
    "        context_layer = context_layer.view(seqlen, batch_size, -1)\n",
    "\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.214874267578125, mean_diff: 0.00832381658256054\n",
      "min_r_diff: 0.0, max_r_diff: 79489.46875, mean_r_diff: 7.876452922821045\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0009765625, mean_diff: 7.922864824649878e-06\n",
      "min_r_diff: 0.0, max_r_diff: 91.75723266601562, mean_r_diff: 0.009910470806062222\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.214874267578125, mean_diff: 0.008323675021529198\n",
      "min_r_diff: 0.0, max_r_diff: 80899.7109375, mean_r_diff: 8.07060718536377\n"
     ]
    }
   ],
   "source": [
    "q = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o0.npy\")).cuda().bfloat16()\n",
    "k = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o1.npy\")).cuda().bfloat16()\n",
    "v = torch.from_numpy(np.load(f\"{cache_dir}/megatron_decoder.layers.0.self_attention.core_attention_input_o2.npy\")).cuda().bfloat16()\n",
    "\n",
    "attention_mask_megatron = torch.triu(torch.ones((128, 128)), diagonal=1).bool().cuda()\n",
    "attention_mask_hf = torch.triu(torch.ones((128, 128)), diagonal=1).bool()\n",
    "attention_mask_hf = attention_mask_hf.unsqueeze(0).unsqueeze(0)\n",
    "attention_mask_hf = attention_mask_hf.to(device=\"cuda\")\n",
    "attention_mask_hf = torch.zeros_like(attention_mask_hf, dtype=torch.float32).masked_fill(attention_mask_megatron, -10000).bfloat16()\n",
    "\n",
    "attn_te = te.DotProductAttention(num_attention_heads=32, num_gqa_groups=8, kv_channels=128, attention_dropout=0.0, attn_mask_type=\"causal\")\n",
    "output_te = attn_te(q, k, v).transpose(1, 0).cpu().float().numpy()\n",
    "attn_torch = TorchDotProductAttention(kv_channels=128, attention_dropout=0.0)\n",
    "k2 = k[:, :, :, None, :].expand(-1, -1, -1, 4, -1).reshape(128, 1, 32, 128)\n",
    "v2 = v[:, :, :, None, :].expand(-1, -1, -1, 4, -1).reshape(128, 1, 32, 128)\n",
    "output_torch2 = attn_torch(q, k2, v2, attention_mask=attention_mask_megatron).transpose(1, 0).cpu().float().numpy()\n",
    "\n",
    "\n",
    "# S B H D -> B H S D\n",
    "q = q.permute(1, 2, 0, 3)\n",
    "k = k.permute(1, 2, 0, 3)\n",
    "v = v.permute(1, 2, 0, 3)\n",
    "k = k[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "v = v[:, :, None, :, :].expand(-1, -1, 4, -1, -1).reshape(1, 32, 128, 128)\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "# Only enable flash attention backend\n",
    "with sdpa_kernel([SDPBackend.MATH]):\n",
    "    output_torch = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=~attention_mask_megatron, dropout_p=0.0).permute(1, 0, 2, 3).reshape(1, 128, -1).cpu().float().numpy()\n",
    "# results = torch.empty(\n",
    "#     q.size(0)*q.size(1),\n",
    "#     q.size(2),\n",
    "#     k.size(2),\n",
    "#     dtype=q.dtype,\n",
    "#     device=q.device,\n",
    "# )\n",
    "# attn_weights = torch.baddbmm(results, q.reshape(q.size(0)*q.size(1), q.size(2), -1), k.transpose(2, 3).reshape(k.size(0)*k.size(1), k.size(2), -1), beta=0.0, alpha=1.0 / math.sqrt(128))\n",
    "# attn_weights = attn_weights.view(q.size(0), q.size(1), q.size(2), k.size(2))\n",
    "# attn_weights = attn_weights.masked_fill(attention_mask_megatron, -10000)\n",
    "\n",
    "\n",
    "# # upcast attention to fp32\n",
    "# attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "# output_torch = torch.matmul(attn_weights, v)\n",
    "# output_torch = output_torch.permute(0, 2, 1, 3).reshape(1, 128, -1).cpu().float().numpy()\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch2)\n",
    "inspect_output(output_torch, output_torch2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082487804349512\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013991205487400293\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.011625289916992188, mean_diff: 0.0002484717406332493\n",
      "min_r_diff: 0.0, max_r_diff: 0.0076371352188289165, mean_r_diff: 0.0019240325782448053\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013990971492603421\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0077877044677734375, mean_diff: 0.00018516556883696467\n",
      "min_r_diff: 0.0, max_r_diff: 0.004377623554319143, mean_r_diff: 0.0014343515504151583\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n",
      "min_r_diff: 0.0, max_r_diff: 0.006097545847296715, mean_r_diff: 2.326020442922072e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Megatron-LM/.pixi/envs/default/lib/python3.10/site-packages/transformer_engine/pytorch/module/rmsnorm.py:193: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at ../torch/csrc/autograd/init.cpp:787.)\n",
      "  self.activation_dtype = torch.get_autocast_gpu_dtype()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# case I, float32 input\n",
    "\n",
    "input = np.load(f\"{cache_dir}/hf_model.dropout.npy\")\n",
    "input = torch.from_numpy(input).cuda().bfloat16()\n",
    "\n",
    "weight = np.load(f\"{cache_dir}/megatron_rmsnorm_weight.npy\")\n",
    "weight = torch.from_numpy(weight).cuda().bfloat16()\n",
    "\n",
    "hf_rms_norm = Emu3RMSNorm(4096, eps=1e-5)\n",
    "hf_rms_norm.weight.requires_grad = False\n",
    "\n",
    "te_rms_norm = te.RMSNorm(4096, eps=1e-5)\n",
    "te_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm = torch.nn.RMSNorm(4096, eps=1e-5)\n",
    "torch_rms_norm.weight.requires_grad = False\n",
    "\n",
    "torch_rms_norm_2 = TorchRMSNorm(4096, zero_centered_gamma=False, eps=1e-5)\n",
    "torch_rms_norm_2.weight.requires_grad = False\n",
    "\n",
    "hf_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "te_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm.weight = torch.nn.Parameter(weight)\n",
    "torch_rms_norm_2.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        output_hf = hf_rms_norm(input).cpu().float().numpy()\n",
    "        output_te = te_rms_norm(input).cpu().float().numpy()\n",
    "        output_torch = torch_rms_norm(input).cpu().float().numpy()\n",
    "        output_torch_2 = torch_rms_norm_2(input).cpu().float().numpy()\n",
    "inspect_output(output_hf, output_te)\n",
    "inspect_output(output_hf, output_torch)\n",
    "inspect_output(output_hf, output_torch_2)\n",
    "inspect_output(output_te, output_torch)\n",
    "inspect_output(output_te, output_torch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.000244140625, mean_diff: 5.820766091346741e-10\n",
      "min_r_diff: 0.0, max_r_diff: 0.006134953815490007, mean_r_diff: 2.340290450320026e-08\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.015625, mean_diff: 0.00018082429596688598\n",
      "min_r_diff: 0.0, max_r_diff: 0.007812499068677425, mean_r_diff: 0.0013987725833430886\n",
      "hf_array.shape: (1, 128, 4096), megatron_array.shape: (1, 128, 4096)\n",
      "min_diff: 0.0, max_diff: 0.0077877044677734375, mean_diff: 0.0001851654815254733\n",
      "min_r_diff: 0.0, max_r_diff: 0.004377623554319143, mean_r_diff: 0.0014343486400321126\n"
     ]
    }
   ],
   "source": [
    "def fp32_rms_norm(x, weight):\n",
    "    x = x.to(torch.float32)\n",
    "    variance = x.pow(2).mean(-1, keepdim=True)\n",
    "    weight = weight.to(torch.float32)\n",
    "    x = x * torch.rsqrt(variance + 1e-5)\n",
    "    return (weight * x).bfloat16()\n",
    "output_fp32 = fp32_rms_norm(input, weight).cpu().float().numpy()\n",
    "inspect_output(output_fp32, output_te)\n",
    "inspect_output(output_fp32, output_hf)\n",
    "inspect_output(output_fp32, output_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
